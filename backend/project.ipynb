{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3sWoc1nOS37"
      },
      "outputs": [],
      "source": [
        "# load test datasets\n",
        "!cp /content/drive/Shareddrives/RAI/tests/datasets.zip /content\n",
        "!unzip /content/datasets.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-hAOb7uUWoO",
        "outputId": "47614f0c-efc0-452a-883a-b069ebdbfc08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJatG0TIJ9AW"
      },
      "source": [
        "# 1. DeepFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exolYpoXKGnV"
      },
      "outputs": [],
      "source": [
        "# pre install\n",
        "!pip install deepface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa6YDjacKLHp",
        "outputId": "cefe883f-91d8-49da-8325-ea2c5b2cec86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "facial_expression_model_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
            "To: /root/.deepface/weights/facial_expression_model_weights.h5\n",
            "100%|██████████| 5.98M/5.98M [00:00<00:00, 91.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "neutral\n",
            "happy\n",
            "sad\n",
            "surprise\n",
            "anger\n",
            "fear\n",
            "28.953771289537713\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from deepface import DeepFace\n",
        "import numpy as np\n",
        "from os import walk\n",
        "\n",
        "emo = ['neutral', 'happy', 'sad', 'surprise', 'anger', 'fear']\n",
        "total=0\n",
        "correct=0\n",
        "for i in emo:\n",
        "    filenames = next(walk(f'/content/datasets/{i}/'), (None, None, []))[2]\n",
        "    for j in filenames:\n",
        "        try:\n",
        "            path = f\"/content/datasets/{i}/{j}\"\n",
        "            img_array = np.fromfile(path, np.uint8)\n",
        "            img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "            label = DeepFace.analyze(img,actions=['emotion'])['dominant_emotion']\n",
        "            if label==i : correct+=1\n",
        "            total+=1\n",
        "        except:\n",
        "            continue\n",
        "    print(i)\n",
        "\n",
        "print((correct*100)/total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QILsWQbCKTUz"
      },
      "source": [
        "# 2. DAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkrzrAGqTPvN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/Shareddrives/RAI/models/DAN/networks/')\n",
        "from dan import DAN\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "from os import walk\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JU2xPMoKXvc"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.data_transforms = transforms.Compose([\n",
        "                                    transforms.Resize((224, 224)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "        self.labels = ['neutral', 'happy', 'sad', 'surprise', 'fear', 'disgust', 'anger']\n",
        "\n",
        "        self.model = DAN(num_head=4, num_class=7, pretrained=False)\n",
        "        checkpoint = torch.load('/content/drive/Shareddrives/RAI/models/DAN/affecnet7_epoch6_acc0.6569.pth',\n",
        "            map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "    \n",
        "    def fit(self, path):\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        img = self.data_transforms(img)\n",
        "        img = img.view(1,3,224,224)\n",
        "        img = img.to(self.device)\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            out, _, _ = self.model(img)\n",
        "            _, pred = torch.max(out,1)\n",
        "            index = int(pred)\n",
        "            label = self.labels[index]\n",
        "\n",
        "            return index, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_e_WgGIWmdo",
        "outputId": "c7fec64f-285d-47e0-d1a5-f0cfd579bb17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19.53757225433526\n"
          ]
        }
      ],
      "source": [
        "model = Model()\n",
        "\n",
        "emo = ['neutral', 'happy', 'sad', 'surprise', 'anger', 'fear']\n",
        "total=0\n",
        "correct=0\n",
        "for i in emo:\n",
        "  filenames = next(walk(f'/content/datasets/{i}/'), (None, None, []))[2]\n",
        "  for j in filenames:\n",
        "      image = f\"./datasets/{i}/{j}\"\n",
        "\n",
        "      index, label = model.fit(image)\n",
        "      if label==i : correct+=1\n",
        "      total+=1\n",
        "\n",
        "print((correct*100)/total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5BX9pq6OK8G"
      },
      "source": [
        "#3. CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xqRnd19OOsW"
      },
      "outputs": [],
      "source": [
        "!pip install fer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbfpbhenPcsr"
      },
      "outputs": [],
      "source": [
        "from fer import FER\n",
        "import cv2\n",
        "from os import walk\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1soE4rjGPYxH"
      },
      "outputs": [],
      "source": [
        "emo = ['angry', 'happy','surprise','sad', 'neutral']\n",
        "\n",
        "detector = FER(emotion_model=\"/content/drive/Shareddrives/RAI/model.h5\")\n",
        "for i in emo:\n",
        "    label_dict = {'neutral': 0, 'happy': 0, 'sad': 0, 'surprise': 0, 'angry': 0, 'fear': 0, 'disgust': 0}\n",
        "    second_label_dict = {'neutral': 0, 'happy': 0, 'sad': 0, 'surprise': 0, 'angry': 0, 'fear': 0, 'disgust': 0}\n",
        "    total=0\n",
        "    correct=0\n",
        "    filenames = next(walk(f'./datasets/{i}/'), (None, None, []))[2]\n",
        "    for k in range(100):\n",
        "        j=filenames[k]\n",
        "        path = f\"./datasets/{i}/{j}\"\n",
        "        img_array = np.fromfile(path, np.uint8)\n",
        "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "        emotions = detector.detect_emotions(img=img)\n",
        "        if len(emotions):\n",
        "            emotions = emotions[0][\"emotions\"]\n",
        "            sorted_emotions = sorted(emotions.items(), key = lambda item: item[1], reverse=True)\n",
        "            label = sorted_emotions[0][0]\n",
        "            if sorted_emotions[0][0] in label_dict.keys():\n",
        "                label_dict[sorted_emotions[0][0]] += 1\n",
        "            if sorted_emotions[1][0] in second_label_dict.keys():\n",
        "                second_label_dict[sorted_emotions[1][0]] += 1\n",
        "        #label, score = detector.top_emotion(img)\n",
        "            if label==i : correct+=1\n",
        "        total+=1\n",
        "    if total is not 0:\n",
        "        print(f\"{i}: {(correct*100)/total}\")\n",
        "    print(label_dict)\n",
        "    print(second_label_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.arange(3)\n",
        "emos = ['angry/sad', 'happy', 'surprise']\n",
        "values = [92, 75, 62]\n",
        "\n",
        "plt.bar(x, values, color = [\"lightpink\", \"dodgerblue\", \"paleturquoise\"])\n",
        "plt.xticks(x, emos)\n",
        "plt.title(\"Emotion Classification Evaluation\")\n",
        "plt.ylabel('Accuracy (label match rate)')\n",
        "plt.grid(alpha=.1, linestyle='--')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ZQk-JY7-cF27",
        "outputId": "ff68a549-b2eb-4225-c29f-e639455fd9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZhsVXW339+pru6+AzIagjJcjKgxJk43RuLwoWg+cY4zTmAU4gAOGKd8Dmg0cR6iRgQHMDgPKE4IImg0it6roiAigyAoTsjUd+iqrlrfH2dXd3XdHk7fe6u79q3f+zz1dJ1d56yzzlnVq/ZZe++1FBEYY4wZHoqVVsAYY8zyYsdvjDFDhh2/McYMGXb8xhgzZNjxG2PMkGHHb4wxQ4Ydv1kSkk6S9KoVOO86SSFppE/y/1XSB7q2/1HSNZImJN1d0sWSDuvDeb8q6aidLbdfSDpR0ul9kn0/SZf2Q7aZjTyPPz8kXQXsC7S6mk+NiON28nmOBp4VEffdmXIXON8dgDcADwDqwNXAqcC7gAOAXwL1iJhaBl2uAE6IiC/sRJknArePiKfuLJkLnOtU4MlAo6v5ioi46w7KPZGddA2SAjgkIi7fUVlmabjHny+PiIi1Xa+d6vSXG0l/AVwAXAP8dUTsDjweWA/stgIqHQRcvALn3Zm8uec7skNO3+w62PHvYkg6WtJ3JL1D0o2SrpT096n9Gkm/7w4tSNpd0kck/UHS1ZJeKamQ9JfAScChKdxxY9r/VEmv7zr+GEmXS/qTpDMl3abrs5D0bEmXJV3eK0nzqP5a4H8j4oSIuA4gIi6NiCdHxI1zXOczJF0i6ZZ0jf/c9dk+kr6UzvknSf8jqUifvUzSr9Nxl0o6PLWfKOl0SWOSJoAacGHq+SPpKkkPSu9rKTR0RZKzUdIB6bN3pft8c2q/X2p/CPCvwBPT/bwwtZ8v6VnpfZHu/9XJTh+RtHv6rBPqOkrSryT9UdL/q/7NmHXvvirpuJ62CyU9ZqFrmEPOYZKu7Wnrvk/3kvTdZIfrJL1H0mj67FvpkAvT/XhirzxJf5nuz40qQ22P7Prs1PR9+nKywQWp82AqYMe/a/J3wE+AvYGPAZ8A/ha4PfBU4D2S1qZ93w3sDtwO+D/A04FnRMQlwLOB76be4h69J5H0QOA/gCcA+1GGZj7Rs9vD07n/Ju33f+fR+UHAZ5Zwjb9Psm8FPAN4h6R7pM9eDFwL3JoyJPavQEi6I3Ac8LcRsVvS5apuoRExGRGde3PXiJjLmZwAHAk8NJ3/n4DN6bMfAHcD9qK895+WNB4RZwH/Dnxygd730en1AEp7rAXe07PPfYE7AocDr04/0Evl40l/ACTdmfIJ58sLXcN2nKcFvAjYBzg06fxcgIi4f9rnrul+fLL7QEl14IvA2cCfAccDH0027PAkyg7DnsDllGFCUwE7/nz5fOoJdV7HdH32y4j4cES0gE9Sxsdfl5za2ZRx39tLqlH+87wiIm6JiKuAtwFPq6jDU4APRcQPI2ISeAXlE8K6rn3eGBE3RsSvgPMoHcpc7A1cV/G8RMSXI+KKKPkmpYPo9EyblD9EB0VEMyL+J8rBrBYwBtxZUj0iroqIK6qes4tnAa9MTyQRERdGxPVJr9Mj4vqImIqIt6Xz3XFBaTM8BXh7RFwZEROU9/NJmj2g/dqI2BIRFwIXAguFb/6l5ztyWmo/A7ibpIO6zvu5ZMMdvYZpImJjRHwvybkKeD9l56IK96b84XtjRDQi4hvAl+j6wQLOiIjvpzGfjzL/d8v0YMefL4+OiD26Xqd0ffa7rvdbACKit20tZU+sM4ja4WrgthV1uE33sclZXd9z/G+73m9O552L6ymddSUkHSHpeymUcyNl73uf9PFbKHuAZ6cw0MuTfpcDLwROBH4v6RPdoaklcAAw5w+GpH9JIaibkl67d+m1GLPuZ3o/QvnU0qHq/QR4a8935CiAiLiFsnf/pLTfkZSOc2dcwzSS7pBCbr+VdDPlE89S7sU1EdHuauv9bi7lXpgu7PiHmz9S9o4P6mo7EPh1er/YlK/fdB8raQ1lz/3X8x4xP18HHltlR0ljwGeBtwL7pjDUVwBB6dgi4sURcTvgkcAJnVh+RHwszVI6iPL63rQdul4DbBMCSrHwl1KGtPZMet3U0Ysl3k9KW0wx+4d8Z/Fx4EhJhwLjlE9jVa6hm03A6s5GeoK8ddfn7wN+Tjlz51aUIbf5xnh6+Q1wQGdsJtH93TQ7gB3/EJNCQZ8C3iBpt/TofwLQmaf9O2D/zoDcHHwceIakuyVn/O/ABemxfqm8Bvh7SW+R9OcAkm6vcsC1d3xhlDL88AdgStIRwD90PpT08HSsKJ1WC2hLuqOkByZdt1I++bRZOh8A/k3SISr5G0l7U84+mkp6jUh6NeUYQIffAet6nFk3HwdeJOngNAbTGRPox/TVr1D+yLwunaNzHxa7hm5+AYxLeliKyb+S0i4ddgNuBiYk3Ql4Ts/xv6Mcy5iLCyh78S+VVFe5huIRbDuGZLYDO/58+WKaDdF5nbGdco6n7LldCXybcjDvQ+mzb1BOafytpD/2HhgRXwdeRdn7vo6yF/yk3v2qkGLthwLrgIsl3ZTkbgBu6dn3FuD5lD9aN1DOVz+za5dDKJ8gJoDvAv8VEedROqU3Uj7p/JZy0PAV26Hu29O5z6Z0bB8EVgFfA86idIhXU/64XNN13KfT3+sl/XAOuR8C/hv4FuWaha2U9tleXtrzHZm2YYrnf45yUP1jXccsdg3TRMRNlIO1H6DsiW+iHFTv8C+UtrkFOIVyvKmbE4HT0vjDE3pkNygd/RGU9vov4OkR8fPKV2/mxQu4jDFmyHCP3xhjhgw7fmOMGTLs+I0xZsiw4zfGmCGjLyludzb77LNPrFu3bqXVWBYiAs2bzsYMGrZXfgyTzTZu3PjHiLh1b3sWjn/dunVs2LBhpdVYFiYnJxkbG1t8RzMQ2F75MUw2k3T1XO0O9QwY9Xp9pVUwS8D2yg/bzI5/4Gi3t2chqVkpbK/8sM3s+AeOVqu1+E5mYLC98sM2s+M3xpihw45/wKjVaiutglkCtld+2GZ2/ANHUdgkOWF75YdtZsc/cDSbzZVWwSwB2ys/bDM7fmOMGTrs+AcMP4bmhe2VH7ZZJit3d4ivfmulNVgSWS0tOeL+K63BiuPFQPlhm7nHP3BMhheX5MTk5ORKq2CWiG1mx2+MMUOHHb8xxgwZdvwDxphskpwYliyPuxK2mR3/wNGMWGkVzBLwnPD8sM3s+AeONnb8OeFMj/lhm9nxG2PM0GHHP2DUGY6ScLsKnhOeH7aZHf/A4YfQvHDYID9sMzv+gaPlGH9WuKhHfthmdvzGGDN02PEPGDXH+LPCRT3ywzaz4x84bJC8cKbH/LDN7GcGjqZj/FnhxUD5YZvZ8RtjzNBhxz9gFI7xZ4XDBvlhm9nxDxx12fHnhBcD5YdtZsc/cLgQS164qEd+2GZ2/MYYM3TY8RtjzJBhxz9guBBLXrioR37YZnb8A0fTMf6s8Jzw/LDN7PgHDrv9vHCmx/ywzfrs+CW9SNLFki6S9HFJ45IOlnSBpMslfVLSaD91MMYYM5u+OX5JtwWeD6yPiLsANeBJwJuAd0TE7YEbgGf2S4cccSGWvPCc8Pywzfof6hkBVkkaAVYD1wEPBD6TPj8NeHSfdcgKP4TmhcMG+WGblY65L0TEryW9FfgVsAU4G9gI3BgRU2m3a4HbznW8pGOBYwEOPPDA6UUXtVqNoiimB2iKoqBer89alDE2Nkaz2SwNHG3qiDYzRU5GEGImIVqBGAEaaVvAqAoaMVP6fBTR6pEBMLVEGVPMFFTvlVFDTEWbVmqvIqOOiB4Zxaxrg7qKWQvDxlTQjJglo/v+bCtD1KVtZXTuMWUvqt1uTxe5WLKd5pAxMjKCpFkyRkZGaDQa5f2RGB0dpdFoEFHqOjo6SqvVmiUDYGpqakkypqampvXqlVGr1ajVajQaDRqNBmNjY4vKqNfrRMQsGTt6fwZFRg526paxZcsWRkdHh8JO86HOTdzZSNoT+CzwROBG4NOUPf0TU5gHSQcAX02hoHlZv359bNiwYfsU+eq3tu+4FWIy2vlM6Tzi/iutwYozOTnp6YGZMUw2k7QxItb3tvfTwzwI+GVE/CEimsDngPsAe6TQD8D+wK/7qEN2jDjGnxWdXqbJB9tsCY5f0hpJSyld8yvg3pJWSxJwOPAz4DzgcWmfo4AvLEHmLo/dfl7ISfWywzZbwPFLKiQ9WdKXJf0e+DlwnaSfSXqLpNsvJDgiLqAM7fwQ+Gk618nAy4ATJF0O7A18cCddyy6BC7HkhRcD5YdttvDg7nnA14FXABdFlCN7kvYCHgC8SdIZEXH6fAIi4jXAa3qarwTutUNaG2OM2W4WcvwPSrH5WUTEnygHbT8ryRNidzIuxJIXLuqRH7bZAqGebqcv6b6SnpHe31rSwb37mJ2Dh53ywgOF+WGbVRjclfQayrj8K1JTHZg3vGN2jIZj/FnRmSNu8sE2qzar5x+BRwKbACLiN8Bu/VTKGGNM/6ji+BtRrvIKKKd19lel4cYR/rzw1MD8sM2qhZQ/Jen9lAuvjgH+CfhAf9UaXkZzWbXbRw5610prsBTySi579QtWWoOVp5OuYZhZ1PFHxFslPRi4Gbgj8OqIOKfvmg0pjWjb+WfEqqLBlrYdSU40Go2hd/6LOn5Jb4qIlwHnzNFmdjIe2s2Lmi2WHf3KT5YTVbqWD56j7YidrYgxxpjlYd4ev6TnAM8FbifpJ10f7QZ8p9+KDSujHt7Nik0O82THsId5YOFQz8eArwL/Aby8q/2WtHrX9IEWXsSVE3W1aIQtlhOtVmvoF3EttHL3poi4KiKOjIirKYupBLBW0oHLpuGQ0XLMOCtG1VppFcwS6RQ3GWaqrNx9hKTLgF8C3wSuonwSMMYYkyFVBndfD9wb+EVEHEyZV/97fdVqiHEhlrzY6jBPdgx7mAeqOf5mRFwPFJKKiDgP2KaUlzFDiSNzJkOq/PTdKGkt8C3go6koy6b+qjW8TBHU3OvPhvFiion2UgrTmZVmamqKWm24bValx/8oYDPwIuAs4ArgEf1UyhhjTP9YsMefaux+KSIeALSB05ZFqyHGhVjyohlOr5EbLsSySI8/IlpAW9Luy6TP0ONhp7zwHP788OBuNT8zAfxU0jl0xfYj4vl902qIaRCMudefDWuKBhPtsZVWwyyBRqPB2Nhw26yK4/9cehljjNkFqJKW2XH9ZcR9/bxo22LZ4UIs1Wb1mGXEufjzYrOTtGWHk7TZ8Q8cjWivtApmCawuXLg7N1xs3Y5/4PBC0LwobLHscCGWahW47gC8BDioe/+IeGAf9TLGGNMnqszq+TRwEnAKZbp400dciCUvXIglPxzjr+b4pyLifX3XxAAwBdRXWglTmVFNMRm2WE5MTU1Rrw+3zeaN8UvaS9JewBclPVfSfp221G76QNsx46yoy4PxudFu22YL9fg3Uo41dmIPL+n6LIDb9UspY4wx/WNex5+KrphlxoVY8mJr23lfcsO5eqqVXnyepD26tveU9Nz+qmVMJvh32mRIlXn8x0TEjZ2NiLgBOKZ/Kg03U47xZ8W4plZaBbNEpqZssyqOv6au5BYpR7/nQxljTKZUCXZ9DfikpPen7X+mrMRl+oDLLuZFI4a7hB/A52+4YaVVWBqtFmzevNJaVOLRe+7ZF7lVHP9LgWOB56TtcygXc5k+YDeSF007/vxwBa5KoZ7jI+KkiHhcer0fOL6KcEl7SPqMpJ9LukTSoWkdwDmSLkt/+/OTlikNx/izYo2TtOVHs7nSGqw4VRz/UXO0HV1R/ruAsyLiTsBdgUuAlwPnRsQhwLlp2xhjzDIxb6hH0pHAk4GDJZ3Z9dFuwJ8WE5zq9N6f9CMREQ2gIelRwGFpt9OA84GXLV31XRNH+POiZYvlhwuxLBjj/1/gOmAf4G1d7bcAP6kg+2DgD8CHJd2VciXwC4B9I+K6tM9vgX3nOljSsZRjCxx44IFMTk4CUKvVKIqCZnpcK4qCer0+/TnA2NgYzWazXJodbeqINtBKYZQRhIBm2i4QI8yEWURZEKURMwkURhGtHhkwM/2yqowpZtIy9MqoIeqIyZSTv4qMOiJ6ZBSzrg3qKqZlAoypoBkxS0b3/dlWhqhL28ro3GOgXq/TbrdptVrbZ6cuGWuLUsZkjNAOsaooZUxFwdYYYW0Kr7QRm9ujrCoa1JKum9qj1NViVKWMrTECAeNFOYWvGQWNGJkO0XRkrC4a0ymWN7VHGdXUdDqGre0R0MzUzUbUaEZtWsaqosGWHhkT7VHGNcVIkrGlXadQMNYlYyoKVqdra1GwpV1nTTE5/VMy0R5jXM0eGW3G1Lk/NVpdMsr7MyMjgE09MtrtnWenjgxardKZdqZJFgXUajMhFQnq9XK7kxK5Xod2uzwWoLOoaqkyWq1SzlwyarVSTq8MgE5O/rlkRMzo1SujKMp9uvUYHS3PuZCM3vvTLaOj1xwyttvvJTvNh/qVm1rSeuB7wH0i4gJJ7wJuphwz6F4QdkNELBjnX79+fWzYsGH7FPnqt7bvuBWiEe18qnAdcf++iD3oXX0R2xdWF42sqnBd/YKdLzO7WT3N5swPwICzo7N6JG2MiPW97VVW7t5b0g8kTUhqSGpJurnCOa8Fro2IC9L2Z4B7AL+TtF+SvR/w++qXsevjod28cCGWDHEhlkqDu+8BjgQuA1YBzwLeu9hBEfFb4BpJd0xNhwM/A85kZsD4KOALS9TZGGPMDlApW1FEXC6pFhEtypj9j4BXVDj0eOCjkkaBK4FnUP7YfErSM4GrgSdsn+q7Ji7EkhcTGYV5TCKTME8/qeL4NyfH/WNJb6Yc8K0UhI6IHwPbxJcoe/9mDlyIJS/GNcVWF2LJi1ZrZiB4SKniwJ9GuaD0OGATcADw2H4qNcy4EEtejLgQS364EMviPf6IuDq93QK8tr/qGGOM6TdVZvU8XNKPJP1J0s2Sbqk4q8dsB3XH+LNiS9thnuwY8jAPVIvxvxN4DPDT6NekfzONb3BeFApaNlpe2I1VivFfA1xkp788uBBLXoy5EEt+dFbUDjFV0zJ/RdI3gen1wRHx9r5pZYwxpm9UcfxvACaAcVx5q++4EEteuBBLhtRssyqO/zYRcZe+a2KAigskzMAwFbZYdrgQSyU/8xVJ/9B3TQwwkxHT5EEnM6bJCBdiqeT4nwOcJWmLp3MaY0z+VFnAtdtyKGJK/BCaFy1bLD8c6vG3dtCo55KL3wBewJUlXsBlxz9odFe5MoPPmmJy8Z3MYOEYvx2/MTuCJ99miNeiLlhsfa+FDoyIRQuuG2OMGTwWCnZtpEwdM1enJoDb9UWjIWfMMf6smGiPrbQKZqmMeh3qvI4/Ig5eTkVMSTOCuhxAyIVxNV2IJTempoZ+gLdKWmZJeqqkV6XtAyXdq/+qDScuxJIXLsSSIS7EUmlw97+AQ4Enp+1bqFBs3RhjzGBS5Xnn7yLiHqnAOhFxQ6rBa/qAC7HkhefxZ8iQh3mgWo+/KalGqhEi6daAn5X6hG9sXhQO9eSHp3NWcvz/CZwB7CvpDcC3gX/vq1ZDTMsx/qwYk4t6ZIcLsVTK1fNRSRuBw1PToyPikv6qZYwxpl9UDXatBjrhnlX9U8e4EEteTLoQS364EEul6ZyvBk4D9gL2AT4s6ZX9VmxY8fKtvGi5EEt+eJ1MpR7/U4C7RsRWAElvBH4MvL6fig0rTYIx9/qzYXXR9Ord3JiaGvrVu1W6K7+hrLfbYQz4dX/UMcYY028WStL2bsqY/k3AxZLOSdsPBr6/POoNH4V7+1nhmrsZ4kIsC4Z6NqS/Gymnc3Y4v2/aGOfpyQzn6ckQL+BaMEnbacupiCmZjLYzdGbEmmKSTY7x50WzCfXh/sFe9KdP0iHAfwB3pivWHxFOy2yGHj+fZYhX7lYa3P0w8D5gCngA8BHg9H4qZUwu2IVkiMOplRz/qog4F1BEXB0RJwIP669aw4vDPHnhME+GDHmYB6rN45+UVACXSTqOcirn2v6qNby4EEteuBBLhrgQS6Ue/wsoUzY8H7gn8DTgqH4qNcy4EEteuBBLhrgQS6UkbT9IbyeAZ/RXHWOMMf1moQVcX2SBsauIeGSVE6Rc/huAX0fEwyUdDHwC2JtyjcDTIqKxJK13YVyIJS82uxBLfgx5mAcW7vG/dSed4wXAJcCt0vabgHdExCcknQQ8k3LWkKEsxOLh3XyoqU3bq3fzwtM5F1zA9c0dFS5pf8oZQG8ATpAk4IHM1O89DTgRO/5pWgQj7vVnw5haNMM9yKxotYY+NfNioZ6TgbMiotnz2e2Ao4GrIuJDC8h/J/BSYLe0vTdwY0RMpe1rgdvOc/5jgWMBDjzwQCYnJwGo1WoURUGzWapUFAX1en36c4CxsTGazSbtdhuiTR3RZqa61QhClJkwocyPMwI00raAURU0YmaodRTR6pEBMLVEGVPMDOD2yqghIoLJVICxiow6InpkFLOuDeoqmIyZAa0xFTQjZsnovj/byhB1aVsZnXsM1Ot12u02rVTdaMl26pKxtihlTMYI7RCrilLGVBRsjRHWFmVksI3Y3B5lVdGglnTd1B6lrhajqTLW1hiBgPGi/Mo1o6ARI6zpkbG6aFB0yRjVFPU0cLu1PQKCcZUyGlGjGTXWFA3WFA1aiC09Mibao4xranrwd0u7TqFgrEvGVBSsTtfWomBLu86aYnL6Z3+iPca4mj0y2tNVvyajRqtLRnl/ZmQE5XTTbhnt9s6zU0cGrVY5N34q/VsXRelYk0ykcgplsznT267Xy0HWjoxO+GWpMlqtmcHaXhm1WilnLhkd5pIRMaNXr4yiKPfp1mN0tDznQjJ670+3jI5ec8jYbr+X7DQfC3VVjgFOAN4p6U/AHyhX7q4DrgDeExFfmO9gSQ8Hfh8RGyUdtsB55iQiTqb84WH9+vUxNjZ7vvRi29MXnebFF7BNT7o3/XHv9mjPnPqROWT0Fk5ZTEap1UIyCka0ozLmuLZeGdIsGZXuT6+Mni9WURSM9MRPK9upS8ZEe7aM3rTHvdtb2rNT7DZihEb0yqj1bM+WsblHxmTUmeyOCARMxLYyJqM23ePvlbE16rNGyVoBzTlkdNO7LmBbGcU2TxhLkVEUO89O0zI2by7/9qY67t3udUS12rY976XKmCtev5iM8fHZ551LxmJ6VdFjJ8jYbr+3CAuFen5L2Vt/qaR1wH7AFuAXEbG5guz7AI+U9FDKH4xbAe8C9pA0knr9++MUz7NwtDgvHN/PEK+TqeZnIuKqiPhuRPy4otMnIl4REftHxDrgScA3IuIpwHnA49JuRwHzPjUMI03P48+KThjKZEQn5DLErER35WWUA72XU8b8P7gCOhhjzNCyLNMRIuJ8Uh7/iLgSuNdynDdHXIglL1yIJUNciKVSsfVHpFw9Zhlwnp68cJ6eDPECrkqhnidSJmh7s6Q79VuhYad7yqQZfNYWk4vvZAaLhhMFLOr4I+KpwN0pp3CeKum7ko6VtNsihxpjjBlAqs7quRn4DGWOnf2AfwR+KOn4PupmzMDjOVgZ4nBqpRj/IyWdQTk4WwfuFRFHAHcFXtxf9YYPF2LJCxdiyRAXYqk0q+exlEnVvtXdGBGbJT2zP2oNL81oU7fzz4ZVRZMtztCZFy7EUsnxnwhc19mQtArYNy3qOrdfig0rHtrNi5otlh8uxFIpxv9pZvujVmozxhiTIVUc/0h3oZT0fnSB/c0O4EIseeFCLBniGH8lx/8HSdPVtiQ9Cvhj/1QabvwQmheuuZshDvVUivE/G/iopPdQ5vG9Bnh6X7UaYlyIJS9G1domBbQZcFyIpVKx9SuAe0tam7Yn+q6VMcaYvlGpqyLpYcBfAeNKix8i4nV91GtocW8/Lybd28+PIe/tQ7UFXCdR5us5njLU83jgoD7rNbTY7edFO2yx7PDK3UqDu38fEU8HboiI1wKHAnfor1rDiwux5IULsWSIC7FUcvxb09/Nkm4DNCnz9RhjjMmQKgHKL0raA3gL8EPKvFSn9FWrIcaFWPLChVgyxIVYFnb8qQDLuRFxI/BZSV8CxiPipmXRbgjxUGFebPXgbn54cHfhUE9EtIH3dm1P2un3l4Zj/FmxtnBRj+xoelymyjPPuZIeK3ko3BhjdgWqOP5/pkzKNinpZkm3SLq5z3oNLf51zYu2LZYf7sNWWrnrEovLyKhz8WfF5rbzFWaHk7Qt7vgl3X+u9t7CLGbn0Ii2nX9GrCoabLHzz4tmc+idf5UpCS/pej8O3AvYCDywLxoNOR7azYuaLZYfYZtVCfU8ontb0gHAO/umkTHGmL6yPTGFa4G/3NmKmJJRDxZmxSaHefJjyMM8UC3G/25mIhAFcDfKFbymD7TwIq6cqDsff36020O/iKvKN3ZD1/sp4OMR8Z0+6TP0uBBLXrgQS4a4EEslx/8ZYGtEtAAk1SStjojN/VXNGGNMP6i0chdY1bW9Cvh6f9Qx7u3nhXP1ZMiIbVbF8Y93l1tM71f3TyVjMsIzA02GVHH8myTdo7Mh6Z7Alv6pNNxM2ZNkxXjhoh7Z4UIslWL8LwQ+Lek3lKlk/pyyFKMxxpgMqbKA6weS7gTcMTVdGhHOa9onXIglL5ouxJIfLsRSqdj684A1EXFRRFwErJX03P6rNpx42CkvPJUzQ4Z8KidUi/EfkypwARARNwDH9E+l4caFWPJijQux5IcLsVRy/LXuIiySasCi69QlHSDpPEk/k3SxpBek9r0knSPpsvR3z+1X3xhjzFKp4vjPAj4p6XBJhwMfT22LMQW8OCLuDNwbeJ6kOwMvp6zjewjlGoGXb5/quyaO8OeFC7FkiAuxVAopvww4FnhO2j4HOGWxgyLiOuC69P4WSZcAtwUeBRyWdjsNOD+dw+BCLLnhQiwZ4iRtlWb1tIGT0gtJ9wPeDTyv6kkkrQPuDlwA7Jt+FAB+C+w7zzHHUv7gcOCBBzI5OQlArVajKAqaKU5XFAX1en36c4CxsTGazSbtdhuiTR3RpsyDU160ENBM2wVihJn4uigdcODynfMAAAroSURBVCPa0xH3UUSrRwbMzLuvKmMKaM8jo4ZoRXu6R1JFRh0RPTKKWdcGdRVMRnvm/qigGTFLRvf92VaGqEvbyujcY6Ber9Nut2m1Wttnpy4Za4tSxmSM0A6xqihlTEXB1hiZLnDeRmxuj7KqaEznxd/UHqWuFqMqZWyNEYiZ+fbNKGjEyHRsviNjddGg6JIxqinqKvXa2h4BwbhKGY2o0Ywaa4oGq4oGE+0xtvTImGiPMq4pRpKMLe06hYKxLhlTUbA6XVuLgi3tOmuKyelniIn2GONq9shoM6bO/anR6pJR3p8ZGQFs6pHRbu88O3Vk0GqV39nO/PiiKAdQO7F0qXS2zeZMLvx6vUyW1pHRWU27VBmtVilnLhm1WimnV8bmzTP7ziUjYkavXhlFUe7TrcfoaHnOhWT03p9uGR295pCx3X4v2Wk+Kk1JkHR34EjgCcAvgc9VOS4duxb4LPDCiLi5u2Z7RISkOUczI+Jk4GSA9evXx9jY2KzPF9uevujUgy7YNh3C2CLbvb3vkTlk1JYoo9RqfhktxFjPMUuVMZcevTLr0iwZle5Pr4yeL1ZRFIz0LIevbKcuGRPt2TIm2mMLbvdWwGrEyDazbSbatZ7t2TJ6e+6TUWey+5sZMBFzy+icv1fG1qjPWtnbCmjOI6PDpp7tbWUUNLe5tuoyimLn2WlaxuaUtmu05+mnd7vXEdVq286wWaqMudIvLCZjZGR221wyFtOrih47QcZ2+71FmNfxS7oDpbM/Evgj8ElAEfGASpJLGXVKp//RiOj8WPxO0n4RcZ2k/YDfV5VnjDFmx1kooPxzyvKKD4+I+0bEuynTxVcizQT6IHBJRLy966MzgaPS+6OALyxN5V0bF2LJCxdiyRDH+Bd0/I+hHJw9T9IpaUbPUrzSfYCnAQ+U9OP0eijwRuDBki4DHpS2TcJZRPJiVLZYdrQq9193WeYN9UTE54HPS1pDORPnhcCfSXofcEZEnL2Q4Ij4NvP/UBy+nfru8pQDru7150Jd7dljAWbwabcX32cXZ9G5gxGxKSI+loqu7w/8CE+/NMaYbFnSpPGIuCEiTo4I99j7hAux5MXWtnP1ZIcLsSzN8RtjevDvtMkQO/4Bw4VY8mLcg7v54UIsdvzGGDNs2PEPGL0rcM1g0wjnds8O5+O34x80/JXMi94UDCYDXIHLjn/QcCGWvHAhlgxxIRY7fmOMGTbs+AcMR/jzomWL5YcLsdjxDxouxJIXvSmhTQY4SZsd/6DRCOcRyYnVjvHnh2P8dvyDhod286KwxfIjbDM7fmOMGTLs+AcMF2LJiwnH+PPDMX47/kHDWUTywrl6MsSFWOz4B422Y8ZZMSIPxmeHC7HY8RtjzLBhxz9g1B3jz4otbceLs8OFWOz4Bw0HevKikC2WHZ7Oacc/aLgQS16MeXA3Pzy4a8dvjDHDhh3/gOFCLHnhQiwZ4kIsdvyDhg2SF1Nhi2WHC7HYzwwaTcf4s2J14YRf2eEkbXb8xhgzbNjxDxg2SF60bLH8cKjH39pBo+5CLFnhBVwZ4gVcdvyDxqQLsWTFmmJypVUwS8Uxfjt+Y3YET77NEK/cteM3xphhw45/wBhzjD8rJtpjK62CWSqjLp5jLzNgNP0YmhXjcrw4O6acX8mOf8BwIZa8cCGWDHEhFjt+Y4wZNuz4BwwXYskLz+PPEM/jXxnHL+khki6VdLmkl6+EDoOKH0LzonCoJz88jrb8jl9SDXgvcARwZ+BISXdebj0GlZZj/FkxJhf1yA4XYlmRHv+9gMsj4sqIaACfAB61AnoYY8xQshLBrtsC13RtXwv8Xe9Oko4Fjk2bE5IuXQbdBoF9gD+utBKmMlnZSy9caQ0GgqxstoMcNFfjwI5yRMTJwMkrrcdyI2lDRKxfaT1MNWyv/LDNVibU82vggK7t/VObMcaYZWAlHP8PgEMkHSxpFHgScOYK6GGMMUPJsod6ImJK0nHA14Aa8KGIuHi59Rhghi68lTm2V34Mvc0UntNqjDFDhVfuGmPMkGHHb4wxQ4YdfwZI+qqk/XdQxlWS9tlZOu1qSFon6aKV1sOsHJK+ImmPldZjORjYefzDgqSRiJg3QbikVcDeEXHtMqplTPYs9r/VtZ8oxzsfugxqDQTu8S8RSZ+XtFHSxWl1MZImJL1B0oWSvidp39T+F2n7p5JeL2kitR8m6X8knQn8TNLrpJk1lUnWC9LmYcD5qf2Nkn4m6SeS3praHiHpAkk/kvT1rnPvLenspOcHcHnYKtQknZLu2dmSVkk6RtIPkm0/K2k1gKRTJZ0kaYOkX0h6eGo/WtIXJJ0v6TJJr0ntC9nYLICkNZK+nGxwkaQndj/BSlov6fz0/kRJ/y3pO8B/L2CPdSlR5EeAi4ADOjLnOl865p6Svpn+/78mab+VuSM7gYjwawkvYK/0dxXlF2ZvIIBHpPY3A69M778EHJnePxuYSO8PAzYBB6ftdcAP0/sCuIKylw/wn8AD03kuZWYm1h7p755dbc8C3tZ13KvT+4clHfdZ6fs3qK9kgyngbmn7U8BTO3ZIba8Hjk/vTwXOSvY6hDL1yDhwNHBdslfnO7J+IRv7tahtHguc0rW9O3BV5/uc7u/56f2JwEZgVdpeyB5t4N5dcq+iTOcw1/nqwP8Ct05tT6Scir7i92d7Xu7xL53nS7oQ+B7lCuRDgAalk4fyS7cuvT8U+HR6/7EeOd+PiF8CRMRVwPWS7g78A/CjiLg+7Xcf4NvATcBW4IOSHgNsTp/vD3xN0k+BlwB/ldrvD5ye5H8ZuGGHrno4+GVE/Di979jxLunp7KfAU5i5vwCfioh2RFwGXAncKbWfExHXR8QW4HPAfRexsVmYnwIPlvQmSfeLiJsW2f/MdO87bGOP1H51RHyv4vnuCNwFOEfSj4FXUv7vZYlj/EtA0mHAg4BDI2JzerwcB5qRugFAi2r3dVPP9gcoeyd/Dnwone92wDVRZjFF0r2Aw4HHAcdRPgm8G3h7RJyZ9Dtx+67OAJNd71uUPcRTgUdHxIWSjqZ8WuvQuwgmFmnfxsZmcSLiF5LuATwUeL2kcymfzjod1/GeQ3r/t+azR+9+C53vDODiiDh0Oy9joHCPf2nsDtyQnP6dgHsvsv/3KB8boUxNsRBnAA8B/pZyVTOUNQvOApC0Ftg9Ir4CvAi4a5dOnVxHR3XJ+xbw5HTsEZQhIbN0dgOuk1Sn7PF383hJhaS/AG5HGYqDsre4VxqYfzTwndQ+l43NIki6DbA5Ik4H3gLcgzIsc8+0y2PnObTDfPZYyvkuBW4t6dC0T13SXy0gZqBxj39pnAU8W9IllF+EuR4Tu3khcLqk/5eOnfcRNSIaks4DboyITqWIhwDHp/e7AV+QNE45UHtCaj8R+LSkG4BvAAen9tcCH5d0MWVs8leVr9J08yrgAuAP6e9uXZ/9Cvg+cCvg2RGxVRKp7bOUoYDTI2IDzGtjszh/DbxFUhtoAs+hfBr7oKR/I01+WIBt7CFp3VLOl2z3OOA/Je1O6TvfCWSZbsYpG/pImgGyJSJC0pMoB3rnLDojqQB+CDw+Ii6TNAZ8J4Y8feygIulU4EsR8Zme9qOB9RFx3BzHzLLxcug57Cxkj2HGoZ7+ck/gx5J+AjwXePFcO6ksPXk5cG7HIUTEpJ3+rsNcNjZmpXCP3xhjhgz3+I0xZsiw4zfGmCHDjt8YY4YMO35jjBky7PiNMWbI+P8EuxUCkzWyTgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGDOvRRBFS3I",
        "outputId": "4ac49cd0-f328-4c96-baad-a2ea303ce800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neutral': 0, 'happy': 0, 'sad': 0, 'surprise': 0, 'angry': 0, 'fear': 0}\n"
          ]
        }
      ],
      "source": [
        "emo = ['neutral', 'happy', 'sad', 'surprise', 'anger', 'fear']\n",
        "detector = FER()\n",
        "\n",
        "second_label_dict = {'neutral': 0, 'happy': 0, 'sad': 0, 'surprise': 0, 'angry': 0, 'fear': 0}\n",
        "total=0\n",
        "correct=0\n",
        "i = 'anger'\n",
        "filenames = next(walk(f'./datasets/{i}/'), (None, None, []))[2]\n",
        "for j in filenames:\n",
        "    path = f\"./datasets/{i}/{j}\"\n",
        "    img_array = np.fromfile(path, np.uint8)\n",
        "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "    emotions = detector.detect_emotions(img=img)\n",
        "    if len(emotions):\n",
        "        emotions = emotions[0][\"emotions\"]\n",
        "        sorted_emotions = sorted(emotions.items(), key = lambda item: item[1], reverse=True)\n",
        "        label = sorted_emotions[0][0]\n",
        "        if sorted_emotions[1][0] in second_label_dict.keys():\n",
        "                second_label_dict[sorted_emotions[1][0]] += 1\n",
        "        #label, score = detector.top_emotion(img)\n",
        "        if label=='angry' : correct+=1\n",
        "    total+=1\n",
        "if total is not 0:\n",
        "    print(f\"{i}: {(correct*100)/total}\")\n",
        "print(second_label_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db48JE7Zczs8"
      },
      "source": [
        "#4. FineTuning CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxJoQSyBdH4l"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install fer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFR1jyPPcuHs"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "import cv2\n",
        "import numpy as np\n",
        "from os import walk\n",
        "from fer import FER\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBB2fob9dazH"
      },
      "outputs": [],
      "source": [
        "model_path =   \"/content/drive/Shareddrives/RAI/model.h5\"\n",
        "base_model = load_model(model_path, compile = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTXEUvS5Gz9V"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIpydPGUwMNj"
      },
      "outputs": [],
      "source": [
        "#If you want to see model layers\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qgoX5n5NYci"
      },
      "outputs": [],
      "source": [
        "# 기존 모델에 argmax layer가 추가된 새로운 모델\n",
        "inputs = tf.keras.Input(shape=(64, 64, 1))\n",
        "x = base_model(inputs, training = False)\n",
        "outputs = tf.keras.layers.Lambda(lambda x: tf.expand_dims(tf.keras.backend.cast(tf.math.argmax(x, axis=1), dtype='float32'),1))(x)\n",
        "new_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "new_model.compile(optimizer=keras.optimizers.Adam(1e-10),  # Very low learning rate\n",
        "               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "new_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHXKxuiQOeMa"
      },
      "outputs": [],
      "source": [
        "# functions for preprocessing (image -> tensor)\n",
        "def __preprocess_input(x):\n",
        "  x = x.astype(\"float32\")\n",
        "  x = x / 255.0\n",
        "  x = x - 0.5\n",
        "  x = x * 2.0\n",
        "  return x\n",
        "\n",
        "def __check_image(path):\n",
        "  img_array = np.fromfile(path, np.uint8)\n",
        "  img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "  is_exact_image = is_base64_img = is_url_img = False\n",
        "  if type(img).__module__ == np.__name__:\n",
        "      is_exact_image = True\n",
        "  elif img is None:\n",
        "      raise InvalidImage(\"Image not valid.\")\n",
        "  elif len(img) > 11 and img[0:11] == \"data:image/\":\n",
        "      is_base64_img = True\n",
        "  elif len(img) > 11 and img.startswith(\"http\"):\n",
        "      is_url_img = True\n",
        "\n",
        "  if is_base64_img:\n",
        "      img = loadBase64Img(img)\n",
        "  elif is_url_img:\n",
        "      img = pil_to_bgr(Image.open(requests.get(img, stream=True).raw))\n",
        "  elif not is_exact_image:  # image path passed as input\n",
        "      if not os.path.isfile(img):\n",
        "          raise ValueError(f\"Confirm that {img} exists\")\n",
        "      img = cv2.imread(img)\n",
        "  return img\n",
        "\n",
        "def __gray_img_to_face(face_coordinates, gray_img):\n",
        "  x, y, width, height = face_coordinates\n",
        "  x_off, y_off = (10, 10)\n",
        "  x1 = x - x_off\n",
        "  x2 = x + width + x_off\n",
        "  y1 = y - y_off\n",
        "  y2 = y + height + y_off\n",
        "\n",
        "  x1 += PADDING\n",
        "  y1 += PADDING\n",
        "  x2 += PADDING\n",
        "  y2 += PADDING\n",
        "  x1 = np.clip(x1, a_min=0, a_max = None)\n",
        "\n",
        "  gray_face = gray_img[max(0, y1) : y2, max(0, x1) : x2]\n",
        "  return gray_face\n",
        "\n",
        "\n",
        "def processImage(path, detector):\n",
        "  gray_faces=[]\n",
        "  img = __check_image(path)\n",
        "  emotion_labels = FER._get_labels()\n",
        "  face_rectangles = detector.find_faces(img, bgr=True)\n",
        "  gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  gray_img = FER.pad(gray_img)\n",
        "  for face_coordinates in face_rectangles:\n",
        "    face_coordinates = FER.tosquare(face_coordinates)\n",
        "    gray_face = __gray_img_to_face(face_coordinates, gray_img)\n",
        "    gray_face = cv2.resize(gray_face, (64, 64))\n",
        "    gray_face = __preprocess_input(gray_face)\n",
        "    gray_faces.append(np.array(gray_face))\n",
        "  return gray_faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX40k7YdfIf_"
      },
      "outputs": [],
      "source": [
        "#preprocessing image\n",
        "PADDING = 40\n",
        "emo={0: \"angry\",\n",
        "    3: \"happy\",\n",
        "    4: \"sad\",\n",
        "    5: \"surprise\",\n",
        "    6: \"neutral\"}\n",
        "detector = FER()\n",
        "faces = {}\n",
        "for i in emo:\n",
        "    total=0\n",
        "    correct=0\n",
        "    face = []\n",
        "    filenames = next(walk(f'./datasets/{emo[i]}/'), (None, None, []))[2]\n",
        "    print(len(filenames))\n",
        "    for n in range(100):      \n",
        "        j = filenames[n]\n",
        "        gray_faces = []\n",
        "        path = f\"./datasets/{emo[i]}/{j}\"\n",
        "        img = __check_image(path)\n",
        "        emotion_labels = FER._get_labels()\n",
        "        face_rectangles = detector.find_faces(img, bgr=True)\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        gray_img = FER.pad(gray_img)\n",
        "        for face_coordinates in face_rectangles:\n",
        "          face_coordinates = FER.tosquare(face_coordinates)\n",
        "          gray_face = __gray_img_to_face(face_coordinates, gray_img)\n",
        "          gray_face = cv2.resize(gray_face, (64, 64))\n",
        "          gray_face = __preprocess_input(gray_face)\n",
        "\n",
        "        gray_faces.append(gray_face)\n",
        "        face.append(np.array(gray_faces))\n",
        "        total += 1\n",
        "        if total%100==0 : print(total)\n",
        "    faces[i] = face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXVx4_P5hcDg"
      },
      "outputs": [],
      "source": [
        "# prepare datasets\n",
        "renamed_faces = {'input_1': [], 'label': []}\n",
        "for emo, images in faces.items():\n",
        "    renamed_faces['input_1'].append(images)\n",
        "    renamed_faces['label'].append([emo]*len(images))\n",
        "features = tf.constant(renamed_faces['input_1'])\n",
        "print(features.shape)\n",
        "features=tf.reshape(features, [500,1,64,64,1])\n",
        "labels = tf.constant(renamed_faces['label'])\n",
        "labels=tf.reshape(labels, [500,1])\n",
        "features_dataset = tf.data.Dataset.from_tensor_slices(features)\n",
        "labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
        "dataset = tf.data.Dataset.zip((features_dataset, labels_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCk8Fgx-UJqB"
      },
      "outputs": [],
      "source": [
        "!cp drive/Shareddrives/RAI/training.zip training.zip\n",
        "!unzip training.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdUZIzDHvfp8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('training' + '/element_spec', 'rb') as in_:\n",
        "    es = pickle.load(in_)\n",
        "\n",
        "df = tf.data.experimental.load(\n",
        "    'training', es, compression='GZIP'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8d961tVvpNi"
      },
      "outputs": [],
      "source": [
        "#unzip and prepare x,y for model input\n",
        "i0= df.map(lambda x,y: x)\n",
        "t0 = df.map(lambda x,y: y)\n",
        "t1 = list(t0)\n",
        "i1 = list(i0)\n",
        "tsi=tf.convert_to_tensor(i1[0:6000]+i1[24000:30000])\n",
        "tsi=tf.reshape(tsi, [12000,64,64,1])\n",
        "tst=tf.convert_to_tensor(t1[0:12000])\n",
        "tst=tf.reshape(tst, [12000,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTa5aZ7rodXp"
      },
      "outputs": [],
      "source": [
        "base_model.compile(optimizer=keras.optimizers.Adam(1e-6),  # Very low learning rate\n",
        "               loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyaAL4C93Mq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0932e838-f004-4816-cc33-8af15d987f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 160s 424ms/step - loss: 0.2429 - accuracy: 0.9342\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9e76ce610>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#Finetune fer by model.fit\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "early_stop = EarlyStopping('val_loss', patience=50)\n",
        "model_checkpoint = ModelCheckpoint('fer'+'.{val_accuracy:.2f}.hdf5', 'val_loss', verbose=1,save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(50/4), verbose=1)\n",
        "base_model.fit(x=tsi.numpy(), y=tst.numpy(), epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.save(\"new1.h5\")"
      ],
      "metadata": {
        "id": "jofgIzAdUARD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2YQMpq4605J"
      },
      "source": [
        "#5. backend server\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doJdBFCG9NLQ"
      },
      "outputs": [],
      "source": [
        "!pip install fer\n",
        "!pip install pyngrok==4.1.1\n",
        "!pip install flask-ngrok\n",
        "!pip install pytube\n",
        "!pip install flask_cors\n",
        "!pip install flask-sock\n",
        "!ngrok authtoken 29I9G3TTcFRuHAYBVnjDeeePiZ5_4QCWkaBqkFfK627DSavxU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyPNlmyJK1aR"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube\n",
        "def downloadYouTube(videourl, path):\n",
        "\n",
        "    yt = YouTube(videourl)\n",
        "    yt = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
        "    yt.download(output_path=path, filename='test.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQeIBDn0Iy6V"
      },
      "outputs": [],
      "source": [
        "#extend fer video object to handle websocket\n",
        "#ref: https://github.com/justinshenk/fer/blob/master/src/fer/classes.py\n",
        "\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Optional, Union\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import base64\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from fer import Video\n",
        "from tqdm import tqdm\n",
        "from tqdm.contrib.logging import logging_redirect_tqdm\n",
        "\n",
        "class fVideo(Video):\n",
        "\n",
        "  def fanalyze(\n",
        "        self,\n",
        "        socket, #for websocket connection\n",
        "        detector,  # fer.FER instance\n",
        "        display: bool = False,\n",
        "        output: str = \"csv\",\n",
        "        frequency: Optional[int] = None,\n",
        "        max_results: int = None,\n",
        "        save_fps: Optional[int] = None,\n",
        "        video_id: Optional[str] = None,\n",
        "        save_frames: bool = True,\n",
        "        save_video: bool = True,\n",
        "        annotate_frames: bool = True,\n",
        "        zip_images: bool = True,\n",
        "        detection_box: Optional[dict] = None,\n",
        "    ) -> list:\n",
        "        \"\"\"Recognize facial expressions in video using `detector`.\n",
        "        Args:\n",
        "            detector (fer.FER): facial expression recognizer\n",
        "            display (bool): show images with cv2.imshow\n",
        "            output (str): csv or pandas\n",
        "            frequency (int): inference on every nth frame (higher number is faster)\n",
        "            max_results (int): number of frames to run inference before stopping\n",
        "            save_fps (bool): inference frequency = video fps // save_fps\n",
        "            video_id (str): filename for saving\n",
        "            save_frames (bool): saves frames to directory\n",
        "            save_video (bool): saves output video\n",
        "            annotate_frames (bool): add emotion labels\n",
        "            zip_images (bool): compress output\n",
        "            detection_box (dict): dict with bounding box for subimage (xmin, xmax, ymin, ymax)\n",
        "        Returns:\n",
        "            data (list): list of results\n",
        "        \"\"\"\n",
        "        frames_emotions = []\n",
        "        if frequency is None:\n",
        "            frequency = 1\n",
        "        else:\n",
        "            frequency = int(frequency)\n",
        "\n",
        "        self.display = display\n",
        "        self.save_frames = save_frames\n",
        "        self.save_video = save_video\n",
        "        self.annotate_frames = annotate_frames\n",
        "\n",
        "        results_nr = 0\n",
        "\n",
        "        # Open video\n",
        "        assert self.cap.open(self.filepath), \"Video capture not opening\"\n",
        "        self.__emotions = detector._get_labels().items()\n",
        "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "        pos_frames = self.cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "        assert int(pos_frames) == 0, \"Video not at index 0\"\n",
        "\n",
        "        self.frameCount = 0\n",
        "        height, width = (\n",
        "            int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
        "            int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        )\n",
        "\n",
        "        fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        length = self.cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        assert fps and length, \"File {} not loaded\".format(self.filepath)\n",
        "\n",
        "        if save_fps is not None:\n",
        "            frequency = fps // save_fps\n",
        "        print(\"{:.2f} fps, {} frames, {:.2f} seconds\".format(fps, length, length / fps))\n",
        "\n",
        "        if self.save_frames:\n",
        "            os.makedirs(self.outdir, exist_ok=True)\n",
        "        root, ext = os.path.splitext(os.path.basename(self.filepath))\n",
        "        outfile = os.path.join(self.outdir, f\"{root}_output{ext}\")\n",
        "\n",
        "        if save_video:\n",
        "            self.videowriter = self._save_video(outfile, fps, width, height)\n",
        "        total=int(length/fps)\n",
        "\n",
        "        while self.cap.isOpened():\n",
        "            ret, frame = self.cap.read()\n",
        "            if not ret:  # end of video\n",
        "                break\n",
        "\n",
        "            if frame is None:\n",
        "                continue\n",
        "\n",
        "            if int(self.frameCount % fps) != 0:\n",
        "                self.frameCount += 1\n",
        "                continue\n",
        "\n",
        "            if detection_box is not None:\n",
        "                frame = self._crop(frame, detection_box)\n",
        "\n",
        "            # Get faces and detect emotions; coordinates are for unpadded frame\n",
        "            try:\n",
        "                faces = detector.detect_emotions(frame)\n",
        "            except Exception as e:\n",
        "                print(\"error\")                \n",
        "\n",
        "            # Offset detection_box to include padding\n",
        "            if detection_box is not None:\n",
        "                faces = self._offset_detection_box(faces, detection_box)\n",
        "            img = os.path.join(self.outdir, (video_id or root) + str(self.frameCount) + \".jpg\")\n",
        "            self._increment_frames(frame, faces, video_id, root)\n",
        "            \n",
        "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "                break\n",
        "\n",
        "            if faces:\n",
        "                for f in faces:\n",
        "                  with open(img, 'rb') as ff:\n",
        "                    img_string = base64.b64encode(ff.read()).decode(\"utf-8\")\n",
        "                  f['image'] =  img_string\n",
        "                  f['box'] = f['box'].tolist()\n",
        "                frames_emotions.append(faces)\n",
        "            else:\n",
        "                # in case, model can't find faces: send empty result with image \n",
        "                with open(img, 'rb') as ff:\n",
        "                  img_string = base64.b64encode(ff.read()).decode(\"utf-8\")\n",
        "                frames_emotions.append([{'box':[],\n",
        "                                         \"emotions\":{\n",
        "                                          'angry':0,\n",
        "                                         'disgust':0,\n",
        "                                         'fear':0,\n",
        "                                         'happy':0,\n",
        "                                         'sad':0,\n",
        "                                         'surprise':0,\n",
        "                                         'neutral':0},\n",
        "                                         'image': img_string\n",
        "                                        }])\n",
        "\n",
        "            socket.send(str(int(100*results_nr//total)))\n",
        "            results_nr += 1\n",
        "            if max_results and results_nr > max_results:\n",
        "                break\n",
        "        self._close_video(outfile, save_frames, zip_images)\n",
        "        return frames_emotions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tgDHpSz8DFGU",
        "outputId": "650b7538-e01b-469d-f278-a5c8ea206143"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https:**youtube.com*shorts*hLHFOeK8f3E?feature=share'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = \"https://youtube.com/shorts/hLHFOeK8f3E?feature=share\"\n",
        "s=s.replace('/','*')\n",
        "s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H5fB2vdE7REm"
      },
      "outputs": [],
      "source": [
        "# run backend server (colab)\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, jsonify, request\n",
        "import shutil \n",
        "from flask_sock import Sock\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "sock = Sock(app)\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>Running Flask on Google Colab!</h1>\"\n",
        "#check connection ping-pong\n",
        "@app.route('/ping', methods=['GET'])\n",
        "def method():\n",
        "    if request.method == 'GET':\n",
        "      response = jsonify('pong!')\n",
        "      response.headers.add('Access-Control-Allow-Origin', '*')\n",
        "      return response\n",
        "#websocket connection\n",
        "@sock.route('/video')\n",
        "def handle(sock):\n",
        "    while True:\n",
        "        data = sock.receive()\n",
        "        downloadYouTube(data, '/content/')\n",
        "        video_filename = \"./test.mp4\"\n",
        "        video = fVideo(video_filename)\n",
        "        shutil.rmtree(\"/content/output\") \n",
        "        raw_data = video.fanalyze(socket=sock, detector=detector, save_frames=True, annotate_frames=True, zip_images=False, save_video=False  )\n",
        "        send_data = { \"data\": raw_data}\n",
        "        sock.send(send_data)\n",
        "        break\n",
        "\n",
        "app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iJatG0TIJ9AW",
        "QILsWQbCKTUz"
      ],
      "name": "test_fer_ipynb의_사본 (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}